# AuditPulse Data Pipeline

## Folder Structure

```
ModuleName/
├── inputs/
├── outputs/
├── logs/
├── script.py
├── test_script.py
├── dockerfile
├── requirements.txt
├── environment.yml
└── README.md

```

## Prerequisites

1. **Anaconda**: [Download and install Anaconda](https://www.anaconda.com/download).  
   - After installation, verify it by running:
     ```bash
     conda --version
     ```

2. **Python 3.x**: [Download and install Python](https://www.python.org/downloads/) (if not already included with Anaconda).  
   - Verify the installation by running:
     ```bash
     python --version
     ```

3. **Git**: [Download and install Git](https://git-scm.com/downloads).  
   - Confirm installation by running:
     ```bash
     git --version
     ```

## Installation

1. Clone the repository:
   ```sh
   git clone https://github.com/hakeematyab/AuditPulse.git
   cd AuditPulse
   ```
2. Create an environment & install dependencies
   ```sh
    cd DataProcessing/ModuleName
    conda env create -f environment.yml
    conda activate ModuleName
    pip install -r requirements.txt
   ```

## **Data Acquisition**

### Ground Truth

1. Data Acquisition  
   The data acquisition process in this pipeline focuses on downloading 10-K filings from the SEC EDGAR database for specified company tickers. Here's a breakdown of the key components:  
2. Ticker List:  
   Currently, the tickers are hardcoded in the script.  
   However, the Securities and Exchange Commission (SEC), has a database of publicly listed US companies. We currently have this database in our GCP bucket, and in operation, we plan to periodically extract from this database and add to our ground truth. This will allow for easier updates and management of the ticker list without modifying the code.

3. SEC EDGAR Downloader:  
   We use the sec\_edgar\_downloader library to fetch the 10-K filings:  
   dl \= Downloader("my\_company", "my\_email@example.com", download\_folder=basePath)  
   This initializes the downloader with a user agent and specifies the download folder.

4. Downloading Process:

The process\_tickers() function handles the downloading of 10-K filings

5. File Processing:  
After downloading, the script processes the HTML files:  
   * Extracts text from HTML using BeautifulSoup  
   * Saves the extracted text as a .txt file  
   * Removes specific unnecessary files  
   * To ensure reproducibility, the following dependencies should be added to the requirements.txt file:  
      * sec-edgar-downloader  
      * Beautifulsoup4

6. Future Enhancements:  
   Implemented functionality to retrieve the ticker list from a cloud storage bucket.  
   Add error handling and retries for network operations.  
   Consider implementing parallel downloads to improve efficiency for large ticker lists. This data acquisition component forms the foundation of your pipeline, ensuring that you have the necessary 10-K filing data for further processing and analysis.


### **Compliance Policy:** 

The reports generated by our application have to be verified for compliance. The compliance requirements are given by the Public Company Accounting Oversight Board (PCAOB). Although automation of these standards by webscraping for the latest document is possible, the requirements vary for different companies based on their fiscal years. As such, we have decided to keep this part of the workflow manual/user-provided.



## **Data Preprocessing**

### Ground Truth

The ***Form10KProcessor*** class processes Form 10-K financial documents to extract relevant information and categorize text into predefined audit-related sections. The processing pipeline consists of the following steps:

1\. Data Ingestion

- The class takes an input file path (e.g., a text file containing the Form 10-K content). It then loads the document and prepares it for processing.

2\. Data Cleaning

Tokenization: Uses nltk.sent\_tokenize to break the document into individual sentences.

Normalization: Converts text into a standardized format (e.g., lowercase transformation, punctuation removal).

3\. Text Embedding and Feature Engineering

Sentence Embedding: Uses sentence-transformers/all-MiniLM-L6-v2 to generate high-dimensional vector representations of each sentence.

Semantic Similarity: Computes cosine similarity between sentence embeddings and predefined category embeddings.

Label Mapping: Assigns each sentence to a relevant audit category, such as: Business Overview, Risk Factors, Financial Statement, Management Discussion & Analysis, etc.

4\. Data Transformation

Sentences are grouped by category, allowing structured extraction of key insights from the 10-K document.

The processed output is formatted as a dictionary or JSON structure, enabling easy integration into further analytical workflows.

5\. Memory Optimization

- Uses PyTorch’s CUDA acceleration if available.  
- Employs garbage collection (gc.collect()) to manage memory and prevent leaks during large-scale document processing.

6\. Modularity & Reusability

The class structure ensures modularity:

- The model and categories can be updated easily.  
- The classification logic can be expanded to new categories.  
- Can be reused across different financial document processing tasks by adjusting input formats and category definitions.


### **Compliance Policy:** 

The compliance standards are extremely large documents. Deterministically or manually generating the enforceable compliance guidelines isn’t feasible or scalable. Therefore, we leveraged an LLM API to generate these guidelines. We currently use the Groq API; however, similar LLM API’s could be employed by swapping the respective clients. The enforcement of structured JSON output was ensured with the Pydantic module and retry mechanisms. It consists of the following keys:

* Rule ID: Reference rule identifier.  
* Standard: The section under which the rule falls.  
* Description: General description of the guideline.  
* Enforcement Guidelines: Step by step process to enforce the guideline.

This module also integrates GCP Firestore and GCP Buckets. Most of the configurable parameters, like maximum token length, temperature, and document paths, are stored and retrieved from the GCP Firestore. Documents such as the standards file and prompts, on the other hand, are stored and retrieved from the GCP bucket. This provides us of control over the quality of outputs without the need to repeat the cycle. The entire module turned into a docker image and pushed to GCP. This image will be launched through GCP Cloud run. For a given company, this operation would be launched once per year or when the compliance standards are amended. As such, for automation, a cloud function that would monitor for changes in the active standards document would be suitable. Since this is an isolated operation without dependency on the ground truth collection and processing event and different run schedules, we decided for it not to be a part of the DAG and instead remain as an isolated job.

*If you want to reproduce the results, please reach out, as you would require the service account key to interact with our GCP project.*



## **Test Modules**

Unit tests are written for each module within the data pipeline. As we discover more edge cases along the development, more test cases would be added.



## **Pipeline Orchestration (Airflow DAGs)**

This Airflow DAG automates the extraction and processing of **SEC 10-K filings** with a structured workflow.

**Pipeline Structure:**

1. **Extract 10-K Filings** (`extractor_10K.py`):  
   * Downloads and stores SEC 10-K filings.  
2. **Process Extracted Data** (`processor_10K.py`):  
   * Cleans, tokenizes, and categorizes text using NLP.  
   * Uses `sentence-transformers` for embedding and classification.  
3. **Task Dependencies:**  
   * Extraction **must finish** before processing begins (`task1 >> task2`).

![][image1]



## **Data Versioning with DVC**

Our application consists of a number of services that leverage different data. As such, storing the entire data locally is redundant and not feasible. Hence, we decided to store data on GCP bucket and download it as per the need. Correspondingly, DVC was initialized on this remote GCP bucket.



## **Tracking and Logging**

Logging is incorporated into the various services making up the application, which write to both the standard output and a log file. This log file is then uploaded to a logs folder within the project GCP bucket. This not only helps us debug quickly but also helps us keep track of the various runs and makes it easier to have a log parsing, searching, and visualization framework such as Grafana, Kibana, and Elasticsearch (GKE).



## **Data Schema & Statistics Generation**

Our application will leverage available LLM APIs rather than building our model. In addition, it is generative in nature; as such, there are no fixed labels. However, since it accepts a few inputs, we have measures in place that would validate the inputs for early termination, thereby saving the cost of invalid requests. In addition, we also sanitize these inputs to prevent instances of injection attacks.

We currently plan on accepting the following inputs: Company Name, Company Central Index Key, and Audit Year. Since the application only serves public companies in the US, we utilize this database provided by the SEC to validate the Central Index Key (CIK). This is a unique identifier assigned to public companies in the US. The database also provides us with additional information, such as the ticker and company name. We also validate the audit year for upper and lower bounds and with regular expressions. Finally, we sanitize the inputs to remove additional spaces. In the event that we decide to use additional inputs, such as user-provided links, in addition to deterministic validation methods, we also plan to sanitize the inputs through an LLM to prevent instances of injection attacks before finalizing the inputs. The inputs will be used by our core application to produce the reports.



## **Anomaly Detection & Alerts**

**Key Components of Anomaly Detection & Alerts**

1. **Detection Techniques**  
   * **Rule-Based Detection:** Set predefined thresholds (similarity rate \< 95%) and trigger alerts.  
2. **Alerting Mechanisms**  
   * **Email Notifications:** Send automated alerts when anomalies are detected.  
   * **Dashboard Monitoring:** Use tools like Grafana, Kibana, or Power BI for continuous anomaly visualization.  
3. **Automated Response Actions**  
   * Logging anomalies for further investigation.  
   * Triggering automated data validation and correction scripts.

**Pipeline Flow Optimization**

**Issue on Docker & Airflow**

- Processing entire 10-K filings for multiple tickers exceeds memory limits.  
- Running inside Docker or Airflow often leads to out-of-memory (OOM) crashes.

Solution: Process only one ticker at a time to reduce memory load.

On Google Cloud (GCP) with GPUs, we will be able to process full datasets efficiently.

![][image2]



## **3\. Data Bias Detection Using Data Slicing**

**Detecting Bias in Your Data**

To ensure that our data is not biased, we performed analysis on the cumulative length of the processed 10-K filings  sliced across different sectors. The categorical feature chosen for subgroup analysis was sector, as each company (ticker) is mapped to a specific sector (e.g., Technology, Healthcare, Energy). This allowed us to evaluate whether the dataset is skewed toward or against certain sectors.  
We used cumulative report lengths (total number of lines in JSON files for each sector) as the metric for analysis. A chi-square test was conducted to statistically evaluate whether the observed distribution of cumulative report lengths across sectors deviates significantly from an expected uniform distribution.

Key steps:

1. Mapped tickers to their respective sectors using Yahoo Finance (\`yfinance\`).  
2. Counted the number of lines in each JSON file and aggregated them by sector.  
3. Performed a chi-square test to detect bias in the sector distribution.

Findings:  
The chi-square test revealed significant bias in the dataset.  Indicating that some sectors are overrepresented while others are underrepresented in terms of diversity in audit reports.

**Data Slicing for Bias Analysis**

To analyze performance across subgroups (sectors), we manually implemented data slicing techniques instead of using tools like SliceFinder, TensorFlow Model Analysis (TFMA), or Fairlearn, as these tools are more suited for evaluating machine learning models rather than raw data analysis.

Our slicing approach:

1. Grouped data by sector (e.g., Technology, Healthcare, Energy) using the ticker-to-sector mapping.  
2. Calculated cumulative report lengths for each sector by summing up the number of lines in all JSON files associated with tickers belonging to that sector.  
3. Visualized the sector distribution using bar charts to identify skewness visually.  
4. Conducted a chi-square test to statistically confirm whether the observed distribution deviates significantly from a uniform distribution.

   

This manual slicing approach was appropriate for our use case since we were analyzing raw data rather than model performance metrics.

**Mitigation of Bias**

After detecting significant bias in the dataset, we implemented oversampling as a mitigation technique to balance the representation of sectors based on cumulative report lengths. Oversampling involved duplicating JSON files from underrepresented sectors until their cumulative line counts matched those of overrepresented sectors.

Steps taken:

1. Identified the most represented sector based on cumulative line counts and set its count as the target (\`target\_count\`).  
2. For each underrepresented sector:  
   1. Duplicated existing JSON files until its cumulative line count reached the target.  
   2. Saved oversampled data into a new directory (\`oversampled-sec-filings\`) to preserve the original dataset.  
3. Re-analyzed the dataset after oversampling:  
   1. Recomputed cumulative line counts for each sector.  
   2. Conducted a new chi-square test to confirm that bias had been mitigated.  
   3. Visualized the adjusted sector distribution using bar charts.

	  
Results After Mitigation:

1. The adjusted dataset showed balanced representation across all sectors.  
   2. The chi-square test after oversampling yielded a p-value \> 0.05, indicating no statistically significant bias in the adjusted dataset.

**Document Bias Mitigation Process**

The following steps were taken to detect and mitigate bias:  
	1\.	Bias Detection:

* Metric: Cumulative report lengths (total lines across JSON files per sector).  
* Statistical Test: Chi-square test.  
* Result: Significant bias detected (Chi2 \= 4750.49, p-value \= 0.0).

	2\.	Bias Mitigation:

* Technique: Oversampling underrepresented sectors by duplicating JSON files until their cumulative line counts matched those of overrepresented sectors.  
* Rationale: To ensure balanced representation across all sectors and avoid skewness caused by overrepresented or underrepresented groups.

	3\.	Post-Mitigation Analysis:

* Metric: Cumulative report lengths after oversampling.  
* Statistical Test: Chi-square test.  
* Result: No significant bias detected (Chi2 ≈ 0, p-value \> 0.05).

	4\.	Trade-offs:

* Increased dataset size due to duplication of JSON files.  
* Potential redundancy introduced into the dataset, which may need careful handling during modeling.

	5\.	Visualization:

* Bar charts were used to visualize sector distributions before and after mitigation, providing clear evidence of improved balance across sectors.


By implementing oversampling, we ensured that all sectors are equally represented in terms of cumulative audit report, reducing potential bias in models trained on this data.

Future considerations:

1. Monitor incoming data for similar biases and apply mitigation techniques as needed.  
2. Explore alternative techniques like stratified sampling or weighted analysis if oversampling introduces excessive redundancy into large datasets.
